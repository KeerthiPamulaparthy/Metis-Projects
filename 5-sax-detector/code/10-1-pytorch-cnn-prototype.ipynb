{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Exploration â€“ CNN Prototype, Fit/Predict, Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audiomod\n",
    "import ptmod\n",
    "# from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# import torchvision\n",
    "# from torchvision import transforms, utils\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from collections import OrderedDict, defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As detailed in previous notebook, I pull a datagroup from the DB, then a smaller subgroup for testing, split into train/test, and create dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sax1203_datagroup = audiomod.pull_datagroup_from_db('sax1203')\n",
    "\n",
    "sax1203_datagroup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    53\n",
       "1    47\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull a smaller sample for PoC run\n",
    "sub_datagroup = sax1203_datagroup.sample(100)\n",
    "sub_datagroup.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length: 82\n",
      "Test set length: 18\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = audiomod.tts(sub_datagroup)\n",
    "\n",
    "# this scaling is pretty tiny, but it'll do the trick for a dry run\n",
    "train_sub = ptmod.SpectroDataset(train_df, scaling=0.125)\n",
    "test_sub = ptmod.SpectroDataset(test_df, scaling=0.125)\n",
    "\n",
    "print(\"Train set length:\", len(train_sub))\n",
    "print(\"Test set length:\", len(test_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-pixels going into the first FC layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduce_axis` function in the `ptmod` module computes the number of pixels along a single axis, given original length, filter length, and stride:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptmod.reduce_axis(28,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptmod.reduce_axis(7,5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `cnn_pixels_out` gives us the total number of values that would be fed to the first FC layer, given input dimensions and kernel/stride/filters of each convolutional or max-pooling layer. Consider the 28x28 images in MNIST Fashion and the four layers of the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x 28 x 28\n",
      "10 x 24 x 24\n",
      "10 x 12 x 12\n",
      "20 x 8 x 8\n",
      "20 x 4 x 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_cnn_layers = (\n",
    "    (5,1,10),\n",
    "    (2,2,0),\n",
    "    (5,1,20),\n",
    "    (2,2,0)\n",
    ")\n",
    "\n",
    "ptmod.cnn_pixels_out((1,28,28), mnist_cnn_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the net from the PyTorch tutorials, optimized for a 32x32 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x 32 x 32\n",
      "6 x 28 x 28\n",
      "6 x 14 x 14\n",
      "16 x 10 x 10\n",
      "16 x 5 x 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tutorial_layers = (\n",
    "    (5,1,6),\n",
    "    (2,2,0),\n",
    "    (5,1,16),\n",
    "    (2,2,0)\n",
    ")\n",
    "\n",
    "ptmod.cnn_pixels_out((1,32,32), tutorial_layers, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With even the tiny 1/8-scale spectros, the output grows significantly when using the same cnn layers as the MNIST example in the PyTorch tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x 64 x 54\n",
      "10 x 60 x 50\n",
      "10 x 30 x 25\n",
      "20 x 26 x 21\n",
      "20 x 13 x 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch drops pixels that would require a partial stride to calculate\n",
    "ptmod.cnn_pixels_out((1,64,54), mnist_cnn_layers, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with some different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x 64 x 54\n",
      "10 x 60 x 50\n",
      "10 x 30 x 25\n",
      "20 x 26 x 21\n",
      "20 x 13 x 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_layers_test = (\n",
    "    (5,1,10),\n",
    "    (2,2,0),\n",
    "    (5,1,20),\n",
    "    (2,2,0)\n",
    ")\n",
    "\n",
    "ptmod.cnn_pixels_out((1,64,54), cnn_layers_test, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps reasonable... let's try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This assumes a (1,64,54) tensor\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # (in channels, out channels, kernel, stride=s)\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5, stride=1)\n",
    "        # (2x2 kernel, stride=2 -- stride defaults to kernel)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5, stride=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(2600, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # need to reshape for fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model, set loss criterion and optimizer\n",
    "cnn_1 = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_1.parameters(), lr=0.01) # set momentum if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc1): Linear (2600 -> 50)\n",
      "  (fc2): Linear (50 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cnn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-instantiate the train loader\n",
    "train_loader = data_utils.DataLoader(train_sub, \n",
    "                                     batch_size=4, \n",
    "                                     shuffle=True,\n",
    "                                     num_workers=2,\n",
    "                                     drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " * Avg loss: 0.692\tTime: 882.398 ms\n",
      "Epoch 2\n",
      " * Avg loss: 0.691\tTime: 733.078 ms\n",
      "Epoch 3\n",
      " * Avg loss: 0.692\tTime: 643.584 ms\n",
      "Epoch 4\n",
      " * Avg loss: 0.691\tTime: 730.601 ms\n",
      "Epoch 5\n",
      " * Avg loss: 0.690\tTime: 744.388 ms\n",
      "Epoch 6\n",
      " * Avg loss: 0.691\tTime: 609.332 ms\n",
      "Epoch 7\n",
      " * Avg loss: 0.690\tTime: 602.612 ms\n",
      "Epoch 8\n",
      " * Avg loss: 0.690\tTime: 607.077 ms\n",
      "Epoch 9\n",
      " * Avg loss: 0.689\tTime: 673.601 ms\n",
      "Epoch 10\n",
      " * Avg loss: 0.688\tTime: 715.653 ms\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 10\n",
    "# minibatches = 5000\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    print(\"Epoch\", epoch+1)\n",
    "    running_loss = 0.0\n",
    "    then = time.perf_counter()\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        sub_then = time.perf_counter()\n",
    "        # separate input data and labels, dump chunk IDs\n",
    "        inputs, labels, _ = data\n",
    "        # wrap in Variable for GD\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # zero parameter gradients, else accumulate\n",
    "        optimizer.zero_grad()\n",
    "        # forward prop\n",
    "        outputs = cnn_1(inputs)\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()         \n",
    "        #verbosity\n",
    "        sub_now = time.perf_counter()\n",
    "        print(\"\\r * {} loss: {:.3f}\\tTime: {:.3f} ms\"\n",
    "              .format(i, loss.data[0], (sub_now-sub_then)*1000), end='')\n",
    "        running_loss += loss.data[0]\n",
    "    now = time.perf_counter()\n",
    "    print(\"\\r * Avg loss: {:.3f}\\tTime: {:.3f} ms\"\n",
    "          .format(running_loss/i, (now-then)*1000))\n",
    "\n",
    "#         # print running loss\n",
    "#         running_loss += loss.data[0]\n",
    "#         if i%minibatches == minibatches:\n",
    "#             # print every 5,000 minibatches or whatever you set 'minibatches' equal to\n",
    "#             print('[%d, %5d] loss: %.3f' % (epoch+1, i, running_loss/minibatches))\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "print('\\nTraining Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5, 5])\n",
      "torch.Size([10])\n",
      "torch.Size([20, 10, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([50, 2600])\n",
      "torch.Size([50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# show learnable parameters for model\n",
    "params = list(cnn_1.parameters())\n",
    "# print(len(params))\n",
    "# print(params[0].size())  # conv1's .weight\n",
    "for param in params:\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5311  0.4689\n",
      " 0.5008  0.4992\n",
      " 0.4878  0.5122\n",
      " 0.5653  0.4347\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view output, should be the predicted probability for each category\n",
    "train_batch = next(iter(train_loader))\n",
    "\n",
    "input_var = Variable(train_batch[0])\n",
    "out = cnn_1(input_var)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"no sax\", \"sax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data_utils.DataLoader(test_sub, \n",
    "                                    batch_size=4, \n",
    "                                    shuffle=False, # set for False for test set\n",
    "                                    num_workers=2,\n",
    "                                    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth\n",
      "4\n",
      "015094: sax\n",
      "008183: sax\n",
      "017245: sax\n",
      "016814: sax\n"
     ]
    }
   ],
   "source": [
    "test_iter = iter(test_loader)\n",
    "spectros, labels, chunk_ids = test_iter.next()\n",
    "\n",
    "# show ground truth\n",
    "print(\"Groundtruth\")\n",
    "print(labels.size(0))\n",
    "for j in range(labels.size(0)):\n",
    "    print(\"{}: {}\".format(chunk_ids[j], classes[labels[j]]))\n",
    "\n",
    "# from PyTorch tutorial\n",
    "# print('GroundTruth: ', ' '.join('%8s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5059  0.4941\n",
      " 0.4927  0.5073\n",
      " 0.5292  0.4708\n",
      " 0.4948  0.5052\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = cnn_1(Variable(spectros))\n",
    "print(outputs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions\n",
      "015094: no sax\n",
      "008183: sax\n",
      "017245: no sax\n",
      "016814: sax\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print(\"Predictions\")\n",
    "for j in range(4):\n",
    "    print(\"{}: {}\".format(chunk_ids[j], classes[predicted[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test spectros: 61 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "results = {}\n",
    "for data in test_loader:\n",
    "    spectros, labels, chunk_ids = data\n",
    "    outputs = cnn_1(Variable(spectros))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    for c_id, gt, pred, out in zip(chunk_ids, labels, predicted, outputs.data):\n",
    "        results[c_id] = (gt, pred, out)\n",
    "\n",
    "print('Accuracy of the network on the test spectros: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'015094': (1, 0, \n",
      " 0.5059\n",
      " 0.4941\n",
      "[torch.FloatTensor of size 2]\n",
      "), '008183': (1, 1, \n",
      " 0.4927\n",
      " 0.5073\n",
      "[torch.FloatTensor of size 2]\n",
      "), '017245': (1, 0, \n",
      " 0.5292\n",
      " 0.4708\n",
      "[torch.FloatTensor of size 2]\n",
      "), '016814': (1, 1, \n",
      " 0.4948\n",
      " 0.5052\n",
      "[torch.FloatTensor of size 2]\n",
      "), '015643': (0, 0, \n",
      " 0.5366\n",
      " 0.4634\n",
      "[torch.FloatTensor of size 2]\n",
      "), '010724': (0, 0, \n",
      " 0.5362\n",
      " 0.4638\n",
      "[torch.FloatTensor of size 2]\n",
      "), '017316': (1, 0, \n",
      " 0.5046\n",
      " 0.4954\n",
      "[torch.FloatTensor of size 2]\n",
      "), '011646': (0, 0, \n",
      " 0.5328\n",
      " 0.4672\n",
      "[torch.FloatTensor of size 2]\n",
      "), '016610': (1, 0, \n",
      " 0.5240\n",
      " 0.4760\n",
      "[torch.FloatTensor of size 2]\n",
      "), '007566': (1, 1, \n",
      " 0.4869\n",
      " 0.5131\n",
      "[torch.FloatTensor of size 2]\n",
      "), '015074': (0, 0, \n",
      " 0.5148\n",
      " 0.4852\n",
      "[torch.FloatTensor of size 2]\n",
      "), '012879': (1, 0, \n",
      " 0.5117\n",
      " 0.4883\n",
      "[torch.FloatTensor of size 2]\n",
      "), '011620': (0, 1, \n",
      " 0.4828\n",
      " 0.5172\n",
      "[torch.FloatTensor of size 2]\n",
      "), '014708': (0, 0, \n",
      " 0.5318\n",
      " 0.4682\n",
      "[torch.FloatTensor of size 2]\n",
      "), '014756': (0, 0, \n",
      " 0.5481\n",
      " 0.4519\n",
      "[torch.FloatTensor of size 2]\n",
      "), '012012': (0, 1, \n",
      " 0.4892\n",
      " 0.5108\n",
      "[torch.FloatTensor of size 2]\n",
      "), '000785': (1, 1, \n",
      " 0.4836\n",
      " 0.5164\n",
      "[torch.FloatTensor of size 2]\n",
      "), '008012': (0, 0, \n",
      " 0.5072\n",
      " 0.4928\n",
      "[torch.FloatTensor of size 2]\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "y_hat = []\n",
    "for val in results.values():\n",
    "    y.append(val[0])\n",
    "    y_hat.append(val[1])\n",
    "\n",
    "print(y)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing Fit and Predict Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(cnn, \n",
    "        dataset, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        num_epochs, \n",
    "        batch_size=4, \n",
    "        minibatches=1):\n",
    "    \"\"\"\n",
    "    Runs feed-forward and back-prop to train CNN model.\n",
    "    ---\n",
    "    IN\n",
    "    cnn: CNN instance \n",
    "    dataset: built SpectroDataset object\n",
    "    optimizer: PyTorch optimizer for back-prop\n",
    "    criterion: PyTorch loss object for loss metric\n",
    "    num_epochs: number of times to cycle through data (int)\n",
    "    batch_size: number of records per batch (int)\n",
    "    minibatches: print loss and time every n minibatches (int)\n",
    "    NO OUT\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = data_utils.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=4, \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch\", epoch+1)\n",
    "        running_loss = 0.0\n",
    "        then = time.perf_counter()\n",
    "        for i, data in enumerate(train_loader, 1):\n",
    "            sub_then = time.perf_counter()\n",
    "            # separate input data and labels, dump chunk IDs\n",
    "            spectros, labels, _ = data\n",
    "            # wrap in Variable for GD\n",
    "            spectros, labels = Variable(spectros), Variable(labels)\n",
    "            # zero parameter gradients, else accumulate\n",
    "            optimizer.zero_grad()\n",
    "            # forward prop\n",
    "            outputs = cnn(spectros)\n",
    "            # calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()         \n",
    "            #verbosity\n",
    "            sub_now = time.perf_counter()\n",
    "            print(\"\\r * {} loss: {:.3f}\\tTime: {:.3f} ms\"\n",
    "                  .format(i, loss.data[0], (sub_now-sub_then)*1000), end='')\n",
    "            running_loss += loss.data[0]\n",
    "    #         running_loss += loss.data[0]\n",
    "    #         if i%minibatches == minibatches:\n",
    "    #             # print every 5,000 minibatches or whatever you set 'minibatches' equal to\n",
    "    #             print('[%d, %5d] loss: %.3f' % (epoch+1, i, running_loss/minibatches))\n",
    "    #             running_loss = 0.0\n",
    "        now = time.perf_counter()\n",
    "        print(\"\\r * Avg loss: {:.3f}\\tTime: {:.3f} ms\"\n",
    "              .format(running_loss/i, (now-then)*1000))\n",
    "    print('\\nTraining Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(cnn, dataset, batch_size=4, res_format='df'):\n",
    "    \"\"\"\n",
    "    Predicts values on trained CNN.\n",
    "    ---\n",
    "    IN\n",
    "    cnn: trained CNN instance\n",
    "    dataset: built SpectroDataset object\n",
    "    batch_size: number of records per batch\n",
    "    res_format: results format, either 'df' for pandas dataframe or 'dict'\n",
    "        for dictionary (str)\n",
    "    OUT\n",
    "    results: if 'dict', dictionary with chunk ID as key, and a tuple of (actual,\n",
    "        predicted, output_array) as value (dict); if 'df', pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    test_loader = data_utils.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=4, \n",
    "        shuffle=False, # set for False for test set\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for data in test_loader:\n",
    "        spectros, labels, chunk_ids = data\n",
    "        outputs = cnn_1(Variable(spectros))\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        for c_id, y, y_hat, out in zip(chunk_ids, labels, pred, outputs.data):\n",
    "            results[c_id] = (y, y_hat, out)\n",
    "            \n",
    "    if res_format == 'df':\n",
    "        results = results_to_df(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(results):\n",
    "    \"\"\"\n",
    "    Converts predict results to Pandas dataframe.\n",
    "    ---\n",
    "    IN\n",
    "    results: dictionary generated by results function (dict)\n",
    "    OUT\n",
    "    df: pandas dataframe of results \n",
    "    \"\"\"\n",
    "\n",
    "    cols = ['chunk_id', 'actual', 'pred', 'e0', 'e1']\n",
    "    results_trans = OrderedDict.fromkeys(cols)\n",
    "    for k in results_trans.keys():\n",
    "        results_trans[k] = []\n",
    "\n",
    "    for k, v in results.items():\n",
    "        for col, val in zip(cols, [k, v[0], v[1], v[2][0], v[2][1]]):\n",
    "            results_trans[col].append(val)\n",
    "    \n",
    "    df = pd.DataFrame(results_trans)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " * Avg loss: 0.684\tTime: 770.440 ms\n",
      "Epoch 2\n",
      " * Avg loss: 0.685\tTime: 622.137 ms\n",
      "Epoch 3\n",
      " * Avg loss: 0.683\tTime: 628.895 ms\n",
      "Epoch 4\n",
      " * Avg loss: 0.682\tTime: 733.074 ms\n",
      "Epoch 5\n",
      " * Avg loss: 0.680\tTime: 832.691 ms\n",
      "Epoch 6\n",
      " * Avg loss: 0.681\tTime: 852.088 ms\n",
      "Epoch 7\n",
      " * Avg loss: 0.679\tTime: 620.179 ms\n",
      "Epoch 8\n",
      " * Avg loss: 0.679\tTime: 663.360 ms\n",
      "Epoch 9\n",
      " * Avg loss: 0.677\tTime: 622.065 ms\n",
      "Epoch 10\n",
      " * Avg loss: 0.677\tTime: 609.768 ms\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "fit(cnn_1, \n",
    "    train_sub, \n",
    "    optim.SGD(cnn_1.parameters(), lr=0.01), \n",
    "    nn.CrossEntropyLoss(), \n",
    "    10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = predict(cnn_1, train_sub)\n",
    "test_results = predict(cnn_1, test_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Config of Class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trying for custom input and fit/predict as methods\n",
    "class CNN_cpcpff(nn.Module):\n",
    "    \"\"\"\n",
    "    Pass input params as a dictionary where each item is a layer\n",
    "    and each value is a list, following this convention:\n",
    "    \n",
    "    Convolutional: c1: [kernel, stride, channels_out]\n",
    "    Max Pooling: p1: [kernel, stride]\n",
    "    Fully Connected: f1: [channels_in, channels_out]\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "        params = {\n",
    "            'c1': [5,1,10],\n",
    "            'p1': [2,2],\n",
    "            'c2': [5,1,20],\n",
    "            'p2': [2,2],\n",
    "            'f1': [2600,50],\n",
    "            'f2': [50,2]\n",
    "        }\n",
    "    \n",
    "    All values must be integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, rs=23):\n",
    "        super(CNN_cpcpff, self).__init__()\n",
    "        self.p = params\n",
    "        self.rs = rs\n",
    "        self.seed_gen = torch.manual_seed(self.rs)\n",
    "        # (in channels, out channels, kernel, stride=s)\n",
    "        self.conv1 = nn.Conv2d(1, \n",
    "                               self.p['c1'][2], \n",
    "                               self.p['c1'][0], \n",
    "                               stride=self.p['c1'][1])\n",
    "        # (2x2 kernel, stride=2 -- stride defaults to kernel)\n",
    "        self.pool1 = nn.MaxPool2d(self.p['p1'][0], self.p['p1'][1])\n",
    "        self.conv2 = nn.Conv2d(self.p['c1'][2], \n",
    "                               self.p['c2'][2], \n",
    "                               self.p['c2'][0], \n",
    "                               stride=self.p['c2'][1])\n",
    "        self.pool2 = nn.MaxPool2d(self.p['p2'][0], self.p['p2'][1])\n",
    "        self.fc1 = nn.Linear(self.p['f1'][0], self.p['f1'][1])\n",
    "        self.fc2 = nn.Linear(self.p['f2'][0], self.p['f2'][1])\n",
    "        # self.seed_gen = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # need to reshape for fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def save_myself(self, fname, dir_out='../data'):\n",
    "        \"\"\"\n",
    "        Saves current object as a .pkl file.\n",
    "        ---\n",
    "        fname: filename of choice (str)\n",
    "        dir_out: path to save directory (str)\n",
    "        \"\"\"\n",
    "        \n",
    "        fpath = os.path.join(dir_out, fname + '.p')\n",
    "        with open(fpath, 'wb') as pf:\n",
    "            pickle.dump(self, pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_init = {\n",
    "    'c1': [5,1,10],\n",
    "    'p1': [2,2],\n",
    "    'c2': [5,1,20],\n",
    "    'p2': [2,2],\n",
    "    'f1': [2600,50],\n",
    "    'f2': [50,2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test = CNN_cpcpff(params_init, rs=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_cpcpff (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc1): Linear (2600 -> 50)\n",
      "  (fc2): Linear (50 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cnn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " * Avg loss: 0.695\tTime: 794.006 ms\n",
      "Epoch 2\n",
      " * Avg loss: 0.693\tTime: 646.161 ms\n",
      "Epoch 3\n",
      " * Avg loss: 0.692\tTime: 677.797 ms\n",
      "Epoch 4\n",
      " * Avg loss: 0.691\tTime: 642.974 ms\n",
      "Epoch 5\n",
      " * Avg loss: 0.691\tTime: 618.255 ms\n",
      "Epoch 6\n",
      " * Avg loss: 0.690\tTime: 706.024 ms\n",
      "Epoch 7\n",
      " * Avg loss: 0.688\tTime: 670.740 ms\n",
      "Epoch 8\n",
      " * Avg loss: 0.687\tTime: 684.706 ms\n",
      "Epoch 9\n",
      " * Avg loss: 0.687\tTime: 709.745 ms\n",
      "Epoch 10\n",
      " * Avg loss: 0.686\tTime: 685.959 ms\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "fit(cnn_test, \n",
    "    train_sub, \n",
    "    optim.SGD(cnn_test.parameters(), lr=0.01), \n",
    "    nn.CrossEntropyLoss(), \n",
    "    10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_df = predict(cnn_test, train_sub)\n",
    "res_test_df = predict(cnn_test, test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_test.save_myself('test_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(train_df, test_df, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, recall, and specificity for train and test\n",
    "    predictions.\n",
    "    ### add precision?\n",
    "    ---\n",
    "    IN\n",
    "    train_df: predict results df of train set\n",
    "    test_df: predict results df of test set\n",
    "    OUT\n",
    "    scores: scores bundle\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = defaultdict(list)\n",
    "    score_types = ['acc', 'rec', 'spec']\n",
    "    \n",
    "    for df in [train_df, test_df]:\n",
    "        df_scores = []\n",
    "        df_scores.append(\n",
    "            metrics.accuracy_score(df.actual, df.pred))\n",
    "        df_scores.append(\n",
    "            metrics.recall_score(df.actual, df.pred))\n",
    "        df_scores.append(\n",
    "            metrics.recall_score(df.actual, df.pred, pos_label=0))\n",
    "#         df_scores.append(df[df.actual == df.pred].shape[0] / df.shape[0])\n",
    "#         df_scores.append(df[(df.actual == 1) & (df.pred == 1)].shape[0] /\n",
    "#                          df[df.actual == 1].shape[0])\n",
    "#         df_scores.append(df[(df.actual == 0) & (df.pred == 0)].shape[0] /\n",
    "#                          df[df.actual == 0].shape[0])\n",
    "        for n, s in zip(score_types, df_scores):\n",
    "            scores[n].append(s)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"MODEL SCORES\")\n",
    "        print(\"Score\\tTrain\\tTest\")\n",
    "        print(\"-\" * 24)\n",
    "        for score in score_types:\n",
    "            print(\"{}\\t{:.3f}\\t{:.3f}\".format(\n",
    "                score.capitalize(), \n",
    "                scores[score][0],\n",
    "                scores[score][1])\n",
    "            )\n",
    "        \n",
    "    return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SCORES\n",
      "Score\tTrain\tTest\n",
      "------------------------\n",
      "Acc\t0.512\t0.444\n",
      "Rec\t0.024\t0.000\n",
      "Spec\t1.000\t1.000\n"
     ]
    }
   ],
   "source": [
    "scores = get_scores(res_train_df, res_test_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Into the module with it all and onto the real stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
