{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import komod\n",
    "import ptmod\n",
    "# from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "# from torchvision import transforms, utils\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from collections import OrderedDict, defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset Sub-Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made class in `pytorchmod` that uses MongoDB to access particular test round. Creates spectrograms on the fly through the `__get__()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this scaling is pretty tiny, but it'll do the trick for a dry run\n",
    "foo_train = ptmod.SpectroDataset('test_run', 'train', scaling=0.125)\n",
    "foo_test = ptmod.SpectroDataset('test_run', 'test', scaling=0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length: 81\n",
      "Test set length: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set length:\", len(foo_train))\n",
    "print(\"Test set length:\", len(foo_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk: 000175\n",
      "Label: 0\n",
      "---\n",
      "Min: -0.7899980545043945\n",
      "Max: 1.0\n",
      "Mean: 0.29952806312907243\n",
      "Std: 0.3481502752916145\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 000232\n",
      "Label: 1\n",
      "---\n",
      "Min: -0.6265758275985718\n",
      "Max: 1.0\n",
      "Mean: 0.26637560173741837\n",
      "Std: 0.31313222857154127\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 000248\n",
      "Label: 0\n",
      "---\n",
      "Min: -1.0\n",
      "Max: 1.0\n",
      "Mean: 0.1910784627393022\n",
      "Std: 0.3638404788205222\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 000426\n",
      "Label: 1\n",
      "---\n",
      "Min: -0.5803954601287842\n",
      "Max: 1.0\n",
      "Mean: 0.23331058807737767\n",
      "Std: 0.27653016076763703\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 000460\n",
      "Label: 0\n",
      "---\n",
      "Min: -1.0\n",
      "Max: 1.0\n",
      "Mean: 0.10331488752981638\n",
      "Std: 0.39873085560956295\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 000512\n",
      "Label: 1\n",
      "---\n",
      "Min: -0.9103786945343018\n",
      "Max: 1.0\n",
      "Mean: -0.10920006837233343\n",
      "Std: 0.3085012483265392\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 000619\n",
      "Label: 1\n",
      "---\n",
      "Min: -0.6913891434669495\n",
      "Max: 1.0\n",
      "Mean: 0.15983487910293828\n",
      "Std: 0.2697588379506309\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 001085\n",
      "Label: 0\n",
      "---\n",
      "Min: -1.0\n",
      "Max: 1.0\n",
      "Mean: -0.3013194930440098\n",
      "Std: 0.5545968166020608\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 001250\n",
      "Label: 1\n",
      "---\n",
      "Min: -0.8894174098968506\n",
      "Max: 1.0\n",
      "Mean: 0.01533554343463363\n",
      "Std: 0.3756132628307513\n",
      "Shape: torch.Size([1, 64, 54])\n",
      "\n",
      "Chunk: 001649\n",
      "Label: 1\n",
      "---\n",
      "Min: -0.9715100526809692\n",
      "Max: 1.0\n",
      "Mean: -0.14963211388201805\n",
      "Std: 0.4210883844170563\n",
      "Shape: torch.Size([1, 64, 54])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    item = foo_train[i]\n",
    "    print(\"\\nChunk:\", item[2])\n",
    "    print(\"Label:\", item[1])\n",
    "    print(\"---\")\n",
    "    ptmod.tensor_stats(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(foo_train[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the loader do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_utils.DataLoader(foo_train, batch_size=4, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes an iterable from loader\n",
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoaderIter"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save a batch of data as a variable\n",
    "loader_unit = next(train_iter)\n",
    "type(loader_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list includes training array, corresponding label, and chunk ID\n",
    "len(loader_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# each should include four records\n",
    "for sub in loader_unit:\n",
    "    print(len(sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'>\n",
      "<class 'torch.LongTensor'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# and the types?\n",
    "for sub in loader_unit:\n",
    "    print(type(sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 54])\n",
      "1\n",
      "008532\n"
     ]
    }
   ],
   "source": [
    "# what about some shapes and contents\n",
    "print(loader_unit[0][0].shape)\n",
    "print(loader_unit[1][0]) # breaks with shape because an int has no such property\n",
    "print(loader_unit[2][0]) # same, because str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.LongTensor of size 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_unit[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to the CNN..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-pixels going into the first FC layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduce_axis` function in the `ptmod` module computes the number of pixels along a single axis, given original length, filter length, and stride:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptmod.reduce_axis(28,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptmod.reduce_axis(7,5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `cnn_pixels_out` gives us the total number of values that would be fed to the first FC layer, given input dimensions and kernel/stride/filters of each convolutional or max-pooling layer. Consider the 28x28 images in MNIST Fashion and the four layers of the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x 28 x 28\n",
      "10 x 24 x 24\n",
      "10 x 12 x 12\n",
      "20 x 8 x 8\n",
      "20 x 4 x 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_cnn_layers = (\n",
    "    (5,1,10),\n",
    "    (2,2,0),\n",
    "    (5,1,20),\n",
    "    (2,2,0)\n",
    ")\n",
    "\n",
    "ptmod.cnn_pixels_out((1,28,28), mnist_cnn_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the net from the PyTorch tutorials, optimized for a 32x32 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x 32 x 32\n",
      "6 x 28 x 28\n",
      "6 x 14 x 14\n",
      "16 x 10 x 10\n",
      "16 x 5 x 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tutorial_layers = (\n",
    "    (5,1,6),\n",
    "    (2,2,0),\n",
    "    (5,1,16),\n",
    "    (2,2,0)\n",
    ")\n",
    "\n",
    "ptmod.cnn_pixels_out((1,32,32), tutorial_layers, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With even the tiny 1/8-scale spectros, the output grows significantly when using the same cnn layers as the MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x 64 x 54\n",
      "10 x 60 x 50\n",
      "10 x 30 x 25\n",
      "20 x 26 x 21\n",
      "20 x 13 x 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch drops pixels that would require a partial stride to calculate\n",
    "ptmod.cnn_pixels_out((1,64,54), mnist_cnn_layers, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with some different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x 64 x 54\n",
      "10 x 60 x 50\n",
      "10 x 30 x 25\n",
      "20 x 26 x 21\n",
      "20 x 13 x 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_layers_test = (\n",
    "    (5,1,10),\n",
    "    (2,2,0),\n",
    "    (5,1,20),\n",
    "    (2,2,0)\n",
    ")\n",
    "\n",
    "ptmod.cnn_pixels_out((1,64,54), cnn_layers_test, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps reasonable... let's try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This assumes a (1,64,54) tensor\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # (in channels, out channels, kernel, stride=s)\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5, stride=1)\n",
    "        # (2x2 kernel, stride=2 -- stride defaults to kernel)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5, stride=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(2600, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # need to reshape for fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model, set loss criterion and optimizer\n",
    "cnn_1 = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_1.parameters(), lr=0.01) # set momentum if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc1): Linear (2600 -> 50)\n",
      "  (fc2): Linear (50 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cnn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-instantiate the train loader\n",
    "train_loader = data_utils.DataLoader(foo_train, \n",
    "                                     batch_size=4, \n",
    "                                     shuffle=True,\n",
    "                                     num_workers=2,\n",
    "                                     drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " * Avg loss: 0.693\tTime: 595.553 ms\n",
      "Epoch 2\n",
      " * Avg loss: 0.690\tTime: 648.583 ms\n",
      "Epoch 3\n",
      " * Avg loss: 0.690\tTime: 601.633 ms\n",
      "Epoch 4\n",
      " * Avg loss: 0.689\tTime: 613.672 ms\n",
      "Epoch 5\n",
      " * Avg loss: 0.689\tTime: 558.390 ms\n",
      "Epoch 6\n",
      " * Avg loss: 0.687\tTime: 552.772 ms\n",
      "Epoch 7\n",
      " * Avg loss: 0.687\tTime: 528.488 ms\n",
      "Epoch 8\n",
      " * Avg loss: 0.683\tTime: 530.346 ms\n",
      "Epoch 9\n",
      " * Avg loss: 0.684\tTime: 528.968 ms\n",
      "Epoch 10\n",
      " * Avg loss: 0.684\tTime: 537.246 ms\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 10\n",
    "# minibatches = 5000\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    print(\"Epoch\", epoch+1)\n",
    "    running_loss = 0.0\n",
    "    then = time.perf_counter()\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        sub_then = time.perf_counter()\n",
    "        # separate input data and labels, dump chunk IDs\n",
    "        inputs, labels, _ = data\n",
    "        # wrap in Variable for GD\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # zero parameter gradients, else accumulate\n",
    "        optimizer.zero_grad()\n",
    "        # forward prop\n",
    "        outputs = cnn_1(inputs)\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()         \n",
    "        #verbosity\n",
    "        sub_now = time.perf_counter()\n",
    "        print(\"\\r * {} loss: {:.3f}\\tTime: {:.3f} ms\"\n",
    "              .format(i, loss.data[0], (sub_now-sub_then)*1000), end='')\n",
    "        running_loss += loss.data[0]\n",
    "    now = time.perf_counter()\n",
    "    print(\"\\r * Avg loss: {:.3f}\\tTime: {:.3f} ms\"\n",
    "          .format(running_loss/i, (now-then)*1000))\n",
    "\n",
    "#         # print running loss\n",
    "#         running_loss += loss.data[0]\n",
    "#         if i%minibatches == minibatches:\n",
    "#             # print every 5,000 minibatches or whatever you set 'minibatches' equal to\n",
    "#             print('[%d, %5d] loss: %.3f' % (epoch+1, i, running_loss/minibatches))\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "print('\\nTraining Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([10, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# show learnable parameters for model\n",
    "params = list(cnn_1.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0000  0.1541\n",
      " 0.0000  0.1153\n",
      " 0.0000  0.1214\n",
      " 0.0000  0.1115\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view output, should be the energy for each category\n",
    "train_batch = next(iter(train_loader))\n",
    "\n",
    "input_var = Variable(train_batch[0])\n",
    "out = cnn_1(input_var)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"no sax\", \"sax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data_utils.DataLoader(foo_test, \n",
    "                                    batch_size=4, \n",
    "                                    shuffle=False, # set for False for test set\n",
    "                                    num_workers=2,\n",
    "                                    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth\n",
      "4\n",
      "000727: sax\n",
      "001193: no sax\n",
      "001503: no sax\n",
      "005661: no sax\n"
     ]
    }
   ],
   "source": [
    "test_iter = iter(test_loader)\n",
    "spectros, labels, chunk_ids = test_iter.next()\n",
    "\n",
    "# show ground truth\n",
    "print(\"Groundtruth\")\n",
    "print(labels.size(0))\n",
    "for j in range(labels.size(0)):\n",
    "    print(\"{}: {}\".format(chunk_ids[j], classes[labels[j]]))\n",
    "\n",
    "# from PyTorch tutorial\n",
    "# print('GroundTruth: ', ' '.join('%8s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000  0.1052\n",
      " 0.0000  0.0557\n",
      " 0.0000  0.0874\n",
      " 0.0000  0.0735\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = cnn_1(Variable(spectros))\n",
    "print(outputs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions\n",
      "000727: sax\n",
      "001193: sax\n",
      "001503: sax\n",
      "005661: sax\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print(\"Predictions\")\n",
    "for j in range(4):\n",
    "    print(\"{}: {}\".format(chunk_ids[j], classes[predicted[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 18 test images: 42 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "results = {}\n",
    "for data in test_loader:\n",
    "    spectros, labels, chunk_ids = data\n",
    "    outputs = cnn_1(Variable(spectros))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    for c_id, gt, pred, out in zip(chunk_ids, labels, predicted, outputs.data):\n",
    "        results[c_id] = (gt, pred, out)\n",
    "\n",
    "print('Accuracy of the network on the 18 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'000727': (1, 1, \n",
      " 0.0000\n",
      " 0.1052\n",
      "[torch.FloatTensor of size 2]\n",
      "), '001193': (0, 1, \n",
      "1.00000e-02 *\n",
      "  0.0000\n",
      "  5.5690\n",
      "[torch.FloatTensor of size 2]\n",
      "), '001503': (0, 1, \n",
      "1.00000e-02 *\n",
      "  0.0000\n",
      "  8.7391\n",
      "[torch.FloatTensor of size 2]\n",
      "), '005661': (0, 1, \n",
      "1.00000e-02 *\n",
      "  0.0000\n",
      "  7.3467\n",
      "[torch.FloatTensor of size 2]\n",
      "), '008523': (1, 1, \n",
      " 0.0000\n",
      " 0.1142\n",
      "[torch.FloatTensor of size 2]\n",
      "), '008599': (1, 1, \n",
      " 0.0000\n",
      " 0.1473\n",
      "[torch.FloatTensor of size 2]\n",
      "), '008722': (0, 1, \n",
      " 0.0000\n",
      " 0.1221\n",
      "[torch.FloatTensor of size 2]\n",
      "), '009152': (1, 1, \n",
      " 0.0000\n",
      " 0.1220\n",
      "[torch.FloatTensor of size 2]\n",
      "), '010576': (1, 1, \n",
      " 0.0000\n",
      " 0.1297\n",
      "[torch.FloatTensor of size 2]\n",
      "), '011182': (0, 1, \n",
      " 0.0000\n",
      " 0.1666\n",
      "[torch.FloatTensor of size 2]\n",
      "), '011659': (0, 1, \n",
      "1.00000e-02 *\n",
      "  0.0000\n",
      "  9.6978\n",
      "[torch.FloatTensor of size 2]\n",
      "), '013746': (0, 1, \n",
      "1.00000e-02 *\n",
      "  0.0000\n",
      "  8.8686\n",
      "[torch.FloatTensor of size 2]\n",
      "), '014629': (0, 1, \n",
      " 0.0000\n",
      " 0.1302\n",
      "[torch.FloatTensor of size 2]\n",
      "), '014810': (0, 1, \n",
      " 0.0000\n",
      " 0.1038\n",
      "[torch.FloatTensor of size 2]\n",
      "), '015073': (1, 1, \n",
      " 0.0000\n",
      " 0.1146\n",
      "[torch.FloatTensor of size 2]\n",
      "), '015854': (0, 1, \n",
      " 0.0000\n",
      " 0.1021\n",
      "[torch.FloatTensor of size 2]\n",
      "), '016048': (0, 1, \n",
      "1.00000e-02 *\n",
      "  0.0000\n",
      "  7.9512\n",
      "[torch.FloatTensor of size 2]\n",
      "), '016681': (1, 1, \n",
      "1.00000e-02 *\n",
      "  0.0000\n",
      "  6.8778\n",
      "[torch.FloatTensor of size 2]\n",
      "), '017174': (1, 1, \n",
      " 0.0000\n",
      " 0.1399\n",
      "[torch.FloatTensor of size 2]\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "y_hat = []\n",
    "for val in results.values():\n",
    "    y.append(val[0])\n",
    "    y_hat.append(val[1])\n",
    "\n",
    "print(y)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing Fit and Predict Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(cnn, \n",
    "        dataset, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        num_epochs, \n",
    "        batch_size=4, \n",
    "        minibatches=1):\n",
    "    \"\"\"\n",
    "    Runs feed-forward and back-prop to train CNN model.\n",
    "    ---\n",
    "    IN\n",
    "    cnn: CNN instance \n",
    "    dataset: built SpectroDataset object\n",
    "    optimizer: PyTorch optimizer for back-prop\n",
    "    criterion: PyTorch loss object for loss metric\n",
    "    num_epochs: number of times to cycle through data (int)\n",
    "    batch_size: number of records per batch (int)\n",
    "    minibatches: print loss and time every n minibatches (int)\n",
    "    NO OUT\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = data_utils.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=4, \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False # not sure the merits of doing this or not?\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch\", epoch+1)\n",
    "        running_loss = 0.0\n",
    "        then = time.perf_counter()\n",
    "        for i, data in enumerate(train_loader, 1):\n",
    "            sub_then = time.perf_counter()\n",
    "            # separate input data and labels, dump chunk IDs\n",
    "            spectros, labels, _ = data\n",
    "            # wrap in Variable for GD\n",
    "            spectros, labels = Variable(spectros), Variable(labels)\n",
    "            # zero parameter gradients, else accumulate\n",
    "            optimizer.zero_grad()\n",
    "            # forward prop\n",
    "            outputs = cnn(spectros)\n",
    "            # calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()         \n",
    "            #verbosity\n",
    "            sub_now = time.perf_counter()\n",
    "            print(\"\\r * {} loss: {:.3f}\\tTime: {:.3f} ms\"\n",
    "                  .format(i, loss.data[0], (sub_now-sub_then)*1000), end='')\n",
    "            running_loss += loss.data[0]\n",
    "    #         running_loss += loss.data[0]\n",
    "    #         if i%minibatches == minibatches:\n",
    "    #             # print every 5,000 minibatches or whatever you set 'minibatches' equal to\n",
    "    #             print('[%d, %5d] loss: %.3f' % (epoch+1, i, running_loss/minibatches))\n",
    "    #             running_loss = 0.0\n",
    "        now = time.perf_counter()\n",
    "        print(\"\\r * Avg loss: {:.3f}\\tTime: {:.3f} ms\"\n",
    "              .format(running_loss/i, (now-then)*1000))\n",
    "    print('\\nTraining Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(cnn, dataset, batch_size=4, res_format='df'):\n",
    "    \"\"\"\n",
    "    Predicts values on trained CNN.\n",
    "    ---\n",
    "    IN\n",
    "    cnn: trained CNN instance\n",
    "    dataset: built SpectroDataset object\n",
    "    batch_size: number of records per batch\n",
    "    res_format: results format, either 'df' for pandas dataframe or 'dict'\n",
    "        for dictionary (str)\n",
    "    OUT\n",
    "    results: if 'dict', dictionary with chunk ID as key, and a tuple of (actual,\n",
    "        predicted, output_array) as value (dict); if 'df', pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    test_loader = data_utils.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=4, \n",
    "        shuffle=False, # set for False for test set\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for data in test_loader:\n",
    "        spectros, labels, chunk_ids = data\n",
    "        outputs = cnn_1(Variable(spectros))\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        for c_id, y, y_hat, out in zip(chunk_ids, labels, pred, outputs.data):\n",
    "            results[c_id] = (y, y_hat, out)\n",
    "            \n",
    "    if res_format == 'df':\n",
    "        results = results_to_df(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(results):\n",
    "    \"\"\"\n",
    "    Converts predict results to Pandas dataframe.\n",
    "    ---\n",
    "    IN\n",
    "    results: dictionary generated by results function (dict)\n",
    "    OUT\n",
    "    df: pandas dataframe of results \n",
    "    \"\"\"\n",
    "\n",
    "    cols = ['chunk_id', 'actual', 'pred', 'e0', 'e1']\n",
    "    results_trans = OrderedDict.fromkeys(cols)\n",
    "    for k in results_trans.keys():\n",
    "        results_trans[k] = []\n",
    "\n",
    "    for k, v in results.items():\n",
    "        for col, val in zip(cols, [k, v[0], v[1], v[2][0], v[2][1]]):\n",
    "            results_trans[col].append(val)\n",
    "    \n",
    "    df = pd.DataFrame(results_trans)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " * Avg loss: 0.681\tTime: 543.595 ms\n",
      "Epoch 2\n",
      " * Avg loss: 0.681\tTime: 620.448 ms\n",
      "Epoch 3\n",
      " * Avg loss: 0.678\tTime: 587.303 ms\n",
      "Epoch 4\n",
      " * Avg loss: 0.684\tTime: 574.613 ms\n",
      "Epoch 5\n",
      " * Avg loss: 0.673\tTime: 568.319 ms\n",
      "Epoch 6\n",
      " * Avg loss: 0.679\tTime: 558.119 ms\n",
      "Epoch 7\n",
      " * Avg loss: 0.676\tTime: 565.400 ms\n",
      "Epoch 8\n",
      " * Avg loss: 0.671\tTime: 575.641 ms\n",
      "Epoch 9\n",
      " * Avg loss: 0.667\tTime: 568.968 ms\n",
      "Epoch 10\n",
      " * Avg loss: 0.674\tTime: 568.993 ms\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "fit(cnn_1, foo_train, optim.SGD(cnn_1.parameters(), lr=0.01), nn.CrossEntropyLoss(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = predict(cnn_1, foo_train)\n",
    "test_results = predict(cnn_1, foo_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Config of Class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trying for custom input and fit/predict as methods\n",
    "class CNN_cpcpff(nn.Module):\n",
    "    \"\"\"\n",
    "    Pass input params as a dictionary where each item is a layer\n",
    "    and each value is a list, following this convention:\n",
    "    \n",
    "    Convolutional: c1: [kernel, stride, channels_out]\n",
    "    Max Pooling: p1: [kernel, stride]\n",
    "    Fully Connected: f1: [channels_in, channels_out]\n",
    "    \n",
    "    For example:\n",
    "    \n",
    "        params = {\n",
    "            'c1': [5,1,10],\n",
    "            'p1': [2,2],\n",
    "            'c2': [5,1,20],\n",
    "            'p2': [2,2],\n",
    "            'f1': [2600,50],\n",
    "            'f2': [50,2]\n",
    "        }\n",
    "    \n",
    "    All values must be integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(CNN_cpcpff, self).__init__()\n",
    "        # (in channels, out channels, kernel, stride=s)\n",
    "        self.p = params\n",
    "        self.conv1 = nn.Conv2d(1, \n",
    "                               self.p['c1'][2], \n",
    "                               self.p['c1'][0], \n",
    "                               stride=self.p['c1'][1])\n",
    "        # (2x2 kernel, stride=2 -- stride defaults to kernel)\n",
    "        self.pool1 = nn.MaxPool2d(self.p['p1'][0], self.p['p1'][1])\n",
    "        self.conv2 = nn.Conv2d(self.p['c1'][2], \n",
    "                               self.p['c2'][2], \n",
    "                               self.p['c2'][0], \n",
    "                               stride=self.p['c2'][1])\n",
    "        self.pool2 = nn.MaxPool2d(self.p['p2'][0], self.p['p2'][1])\n",
    "        self.fc1 = nn.Linear(self.p['f1'][0], self.p['f1'][1])\n",
    "        self.fc2 = nn.Linear(self.p['f2'][0], self.p['f2'][1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # need to reshape for fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def save_myself(self, fname, dir_out='../data'):\n",
    "        \"\"\"\n",
    "        Saves current object as a .pkl file.\n",
    "        ---\n",
    "        fname: filename of choice (str)\n",
    "        dir_out: path to save directory (str)\n",
    "        \"\"\"\n",
    "        \n",
    "        fpath = os.path.join(dir_out, fname + '.p')\n",
    "        with open(fpath, 'wb') as pf:\n",
    "            pickle.dump(self, pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_init = {\n",
    "    'c1': [5,1,10],\n",
    "    'p1': [2,2],\n",
    "    'c2': [5,1,20],\n",
    "    'p2': [2,2],\n",
    "    'f1': [2600,50],\n",
    "    'f2': [50,2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test = CNN_cpcpff(params_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_cpcpff (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc1): Linear (2600 -> 50)\n",
      "  (fc2): Linear (50 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cnn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " * Avg loss: 0.693\tTime: 732.572 ms\n",
      "Epoch 2\n",
      " * Avg loss: 0.693\tTime: 802.186 ms\n",
      "Epoch 3\n",
      " * Avg loss: 0.693\tTime: 683.722 ms\n",
      "Epoch 4\n",
      " * Avg loss: 0.693\tTime: 580.796 ms\n",
      "Epoch 5\n",
      " * Avg loss: 0.693\tTime: 582.000 ms\n",
      "Epoch 6\n",
      " * Avg loss: 0.693\tTime: 593.529 ms\n",
      "Epoch 7\n",
      " * Avg loss: 0.693\tTime: 558.362 ms\n",
      "Epoch 8\n",
      " * Avg loss: 0.693\tTime: 601.578 ms\n",
      "Epoch 9\n",
      " * Avg loss: 0.693\tTime: 622.007 ms\n",
      "Epoch 10\n",
      " * Avg loss: 0.693\tTime: 710.040 ms\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "fit(cnn_test, foo_train, optim.SGD(cnn_test.parameters(), lr=0.01, momentum=0.5), nn.CrossEntropyLoss(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_df = predict(cnn_test, foo_train)\n",
    "res_test_df = predict(cnn_test, foo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test.save_myself('test_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(train_df, test_df, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, recall, and specificity for train and test\n",
    "    predictions.\n",
    "    ### add precision?\n",
    "    ---\n",
    "    IN\n",
    "    train_df: predict results df of train set\n",
    "    test_df: predict results df of test set\n",
    "    OUT\n",
    "    scores: scores bundle\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = defaultdict(list)\n",
    "    score_types = ['acc', 'rec', 'spec']\n",
    "    \n",
    "    for df in [train_df, test_df]:\n",
    "        df_scores = []\n",
    "        df_scores.append(\n",
    "            metrics.accuracy_score(df.actual, df.pred))\n",
    "        df_scores.append(\n",
    "            metrics.recall_score(df.actual, df.pred))\n",
    "        df_scores.append(\n",
    "            metrics.recall_score(df.actual, df.pred, pos_label=0))\n",
    "#         df_scores.append(df[df.actual == df.pred].shape[0] / df.shape[0])\n",
    "#         df_scores.append(df[(df.actual == 1) & (df.pred == 1)].shape[0] /\n",
    "#                          df[df.actual == 1].shape[0])\n",
    "#         df_scores.append(df[(df.actual == 0) & (df.pred == 0)].shape[0] /\n",
    "#                          df[df.actual == 0].shape[0])\n",
    "        for n, s in zip(score_types, df_scores):\n",
    "            scores[n].append(s)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"MODEL SCORES\")\n",
    "        print(\"Score\\tTrain\\tTest\")\n",
    "        print(\"-\" * 24)\n",
    "        for score in score_types:\n",
    "            print(\"{}\\t{:.3f}\\t{:.3f}\".format(\n",
    "                score.capitalize(), \n",
    "                scores[score][0],\n",
    "                scores[score][1])\n",
    "            )\n",
    "        \n",
    "    return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SCORES\n",
      "Score\tTrain\tTest\n",
      "------------------------\n",
      "Acc\t0.630\t0.526\n",
      "Rec\t0.976\t1.000\n",
      "Spec\t0.256\t0.182\n"
     ]
    }
   ],
   "source": [
    "scores = get_scores(res_train_df, res_test_df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Into the module with it all and onto the real stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
